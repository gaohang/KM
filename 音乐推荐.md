# 核心

## 冷启动

### PlaylistColdStart



## Embedding

### 架构

item序列构建： 

ai-recommed-music： spark node2vec 离线训练，存入hdfs

imusic-faiss：基于faiss和flask搭建 检索服务，将hdfs上的embedding vec 存入gpu服务器内存并构建index (i.e.  voronoi cell)

imusic-rec：在线检索 simUtils.getFaissEmbedding  和 simUtils.getSimItemsByFaissEmbedding

### 实现

item序列构建

word2vec

```
训练样本：对每句话（words序列）进行滑窗
输入向量表达 vs 输出向量表达：最终结果取输入向量表达
减少复杂度1/1000：negative sampling，hierarchical softmax
模型：skipgram，cbow
item2vec：序列构建，或不需要滑窗（i.e. 序列中任意两个item均相关）；负采样，挑选希望区分的样本；
```

airbnb

```滑窗
短期兴趣：一次搜索过程中点进详情页且停留时长>30s的item （利用搜索行为，表达了用户主观意愿，同时学到了文本相关性）
长期兴趣：user过去一年预订的items
滑窗：在将最终预定的item加入每个滑窗中（以此实现 reserved item 的 embedding 与更多的item相似，易被embedding相似召回）
负采样：为了区分同一地区的样本，限定负采样范围为同一地区的房源
冷启动item的embedding：同样类型的item的embedding均值
```

imusic

```
item数量
seq数量
构建方法
```

ai-recommed-music： spark node2vec 离线训练，存入hdfs

```
    val model = new Word2Vec()
      .setInputCol("ids")
      .setOutputCol("result")
      .setVectorSize(vectorSize)
      .setMinCount(2)
      .setNumPartitions(50)
      .setWindowSize(windowSize)
      .setMaxIter(maxIter)
      .setMaxSentenceLength(100)
      .fit(trainData)

    val vecResult = model.getVectors.toDF("id", "feature").map(x => {
      val id = x.getAs[String]("id")
      val features = x.getAs[DenseVector]("feature").values
      (id, features)
    }).toDF("song_id", "feature")
```

imusic-faiss：基于faiss和flask搭建 检索服务，将hdfs上的embedding vec 存入gpu服务器内存并构建index (i.e.  voronoi cell)

```
def update_index_hdfs():
    """
    更新索引从hdfs
    :return:
    """
    quantizer = faiss.IndexFlatL2(dim)
    index = faiss.IndexIVFFlat(quantizer, dim, imusic_config.list_num, faiss.METRIC_INNER_PRODUCT)
    index.train(xb)
    assert index.is_trained
    index.nprobe = get_nprobe(name)
    index.add_with_ids(xb, ids)
    index.set_direct_map_type(faiss.DirectMap.Hashtable)
    # 保存索引到本地
    faiss.write_index(index, imusic_config.index_file_path + name)
```

```
@app.route('/search', methods=['POST'])
def search():
```

imusic-rec：在线检索 

```
Map<String, List<Float>> result7 = SimilarUtils.getFaissEmbedding("song_node2vec",
        Lists.newArrayList("662487", "582782", "102227052"));
```

 

### faiss 索引

http://d0evi1.com/ivf/

Inverted File Index(IVF) 是pre-filtering技术，以便你无需对所有vectors做exhaustive search。实现相当简单：**首先，你使用聚类(比如：kmeans)将数据集聚类生成较大数目（比如：100个）的数据分区(partitions)。接着，在query时，你会将你的query vector与partition centroids进行对比，（例如：找到10个最近的聚类），接着你只在这些分区上对vectors进行搜索**。

在IR中，”inverted index”指的是文本搜索索引，它会将词汇表中的每个词映射到数据文档中的所有位置。看起来有点像textbook的索引，将words映射到page numbers，因而称为inverted index。然而，在我们的上下文中，该技术的意思是，使用k-means聚类将数据集进行划分，以便你可以重定义你的搜索（只需搜索部分分区，忽略其它）。

在构建index时，使用聚类将数据集聚到多个partitions上。在dataset中的每个vector会属于这些clusters/partitions的其中之一。对于每个partition，你都具有一个属于它的所有vectors的list（被称为：inverted file lists）。你具有关于所有这些partition centroids的一个matrix，它会被用于找出哪些partitions会进行search。

按这种方式划分数据集并不完美，因为如果一个query vector落到离它最近cluster的外面，那么它的最近邻很可能停留在多个附近的cluster上。解决该问题的简单方法是，搜索多个partitions。搜索多个附近的partitions很明显会花费更多时间，但它会给出更好的accuracy。在搜索时，你可以比较你的query vector与所有partition centroids来找到离它最近的partition。可以配置多少个。一旦你发现了这些centroids，你只需从这些partitions中选择dataset vectors，使用product quantizer来进行KNN search。

需要注意一些术语：

- verb “probe”指的是，选择partitions进行search。代码中你会看到index参数”nprobe”意味着：有多少partitions进行probe。
- Faiss的作者喜欢使用术语：Voronoi cells（而非“dataset partitions”）。一个Voronoi cell指的是，属于一个cluster的space区域。也就是说，它会包含在space中的所有points（vector与某个cluster的centroid会比其它clusters更接近）。



### 质量评估

#### embedding size

头部item的embedding size应当较大；低频 item反之，否则导致过拟合

#### embedding 相似度

findSynonym(vector, topk) 人工评估，vector可从高中低频item中选取



### User embedding

生成方式

​	用户行为序列 pooling， attention sum， gru

兴趣拆分

- 按category拆分：将不同类embedding按照类别聚类后，对每类做pooling，得到多个类别的embedding。Recall适合拆分，拆分能够缓解高频item类别召回过多，多路召回后需要对每个类别召回的 item list进行截断；Rank不适合。
- 长短期兴趣拆分：例如airbnb，长期：booked item seq；短期：搜索过程点击进入详情页超过30s的item seq
- 多模态融合



### 双塔模型

user 塔：user features -> DNN1 -> embedding

item 塔：item features -> DNN2 -> embedding

通过dot product， triplet loss技术 同时训练DNN1 和 2



# 工程

## hbase

## redis

## hive

## 标签体系

- 标签生成

  对歌单标题进行关键词提取+人工添加
  相似关键词合并 -> 48个基础标签

- 歌单打标

  用 基础标签 对 歌单打标

- 标签传播

  歌单标签 -> 歌曲、用户、专辑 标签
  
- 歌单推荐



![image-20200911110735588](/Users/gaohang/Library/Application Support/typora-user-images/image-20200911110735588.png)



### 画像

#### 用户画像

##### 基础信息

​	age，city，

##### 统计信息

​	<用户，歌单>行为次数和比率

​	长中短期用户行为次数和率

#### 歌单画像

##### 基础信息

​	曲目数：num,free,vip

##### 统计信息

​	长中短期<用户，歌单>次数和率

### 召回

- 术语：seed，阿里称之为trigger，i.e.召回使用的方法，凭什么召回。例如，seed为播放过的歌单，就是以用户歌单播放行为召回。
- 多路召回的实质：每一路相当于用一个user feature和相应的一个 item feature关联。例如，兴趣召回就是用户兴趣和物品兴趣关联

#### 行为召回

- ​	<user, seed 歌单> left join <歌单， 相似歌单列表> => explode 一行变多行 =>  <user, 歌单>
- ​	注：seed 歌单=topK（播放，收藏，分享，搜索）歌单，	行为类别：<用户,歌单>行为，<用户,歌曲>行为

#### 向量召回

#### 场景召回

- 离线节假日召回：以weekday为seed，<user, weekday> left join <weekday，topK歌单>
- 时间地点召回类似上面



#### 标签召回

```scala
object TagPreference {
```



### 排序

#### 特征

##### User 侧

基础特征

统计特征

行为序列



##### Item 侧

基础特征

统计特征

行为序列



##### User Item 交叉



real time 特征

用户最近行为





#### 模型

##### XGB

###### 缺点

- 难挖掘<user,item>交叉特征。虽然在recall时使用了用户行为seed召回，但召回的item若较为冷门，则会被xgb打低分。因此还是要DNN。

###### train

```
正样本：playlistPlayHistory
负样本：trainDataXgbNeg  采样后，正:负=1:3
val trainDataXgb = trainDataXgbPos.union(trainDataXgbNeg)
```

###### pred

<user, item> 数量达百亿，需要利用hash分桶 分批打分。

##### DNN



### 人工规则

模型打分偏向热门歌曲，因此需要人工规则给一些冷门歌单提高分数。、



# 视频推荐





## VIVO视频

user: 30w  coldstart: 20%

Item: 2000w coldstart: 2w

流程：

- 输入（imei, item_id, loc, time）-> 
- (user:[profile, tag prefer, embedding], item:[title, tag, entity, producer, age], session:[time, location]) ->
- recall output:2000 (tag, embedding) -> 
- rank output:2000 ( DNN ) -> 
- 输出 rerank output:8 (按tag打散) 

user冷启动：增加多样性

item冷启动：tag、entity关联

负反馈

  

